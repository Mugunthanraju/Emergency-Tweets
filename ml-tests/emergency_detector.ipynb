{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine All Three For Twitter API Funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watson NLU gives fear scores, and requires 1 API call each time\n",
    "# Sarcasm Detector is a standalone model, and can be pickle loaded.\n",
    "# NLP model for classification. Done after the other two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the three packages and pickle\n",
    "import classes\n",
    "import nlp_model\n",
    "import pickle as pk\n",
    "import watson_nlu\n",
    "import sarcasm_detector\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "['today is gonna be a good good day nope sike']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ekremguzelyel/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiate this in the first place.\n",
    "nlu_api = watson_nlu.get_ibm_nlu()\n",
    "clf, tv, ps, scores = sarcasm_detector.get_model()\n",
    "\n",
    "\n",
    "tweet = \"Today is gonna be a good good day. #Nope #Sike\"\n",
    "# Given a tweet as tweet.text (Later on I'll check a list of tweets. )\n",
    "print(watson_nlu.is_fearful(tweet, nlu_api))\n",
    "processed_tweets = sarcasm_detector.preprocess([tweet],ps)\n",
    "print(processed_tweets)\n",
    "clf.predict(tv.transform(processed_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ekremguzelyel/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "# I know this is sarcastic -> Discard and Not Fearful -> Discard\n",
    "fear_matrix = [watson_nlu.is_fearful(tweet, nlu_api) for tweet in tweets]\n",
    "processed_tweets = sarcasm_detector.preprocess([tweet], ps)\n",
    "sarcasm_matrix = clf.predict(tv.transform(processed_tweets))\n",
    "result_matrix = [1 if (fear_matrix[i] and sarcasm_matrix[i]) else 0 for i in range(len(fear_matrix))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_eligible(tweets):\n",
    "    \"\"\"Given list of tweets as text return if emergent/eligible for a test 1, else 0.\n",
    "    If you want to pass only one tweet, add brackets tweet -> [tweet]\n",
    "    Returns: \n",
    "        1-0 matrix of len(tweets)\n",
    "    \"\"\"\n",
    "    fear_matrix = [watson_nlu.is_fearful(tweet, nlu_api) for tweet in tweets]\n",
    "    processed_tweets = sarcasm_detector.preprocess(tweets, ps)\n",
    "    sarcasm_matrix = clf.predict(tv.transform(processed_tweets))\n",
    "    return [1 if (fear_matrix[i] and not sarcasm_matrix[i]) else 0 for i in range(len(fear_matrix))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, True]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = [\"tweet\", \"I am so afraid. The cops just shot a guy. He is bleeding\"]\n",
    "[watson_nlu.is_fearful(tweet, nlu_api) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ekremguzelyel/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "eligible_tweets = [tweet for i, tweet in enumerate([tweet]) if is_eligible([tweet])[i]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eligible_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emergent_tweets(tweets):\n",
    "    \"Returns: Emergent tweets with their corresponding Class.\"\n",
    "    eligible_tweets = [tweet for i, tweet in enumerate(tweets) if is_eligible(tweets)[i]]\n",
    "    return eligible_tweets, nlp_model.classify_list(eligible_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ekremguzelyel/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['I am so afraid. The cops just shot a guy. He is bleeding'], ['police'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emergent_tweets([\"tweet\", \"I am so afraid. The cops just shot a guy. He is bleeding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "\n",
    "# Watson NLU gives fear scores, and requires 1 API call each time\n",
    "# Sarcasm Detector is a standalone model, and can be pickle loaded.\n",
    "# NLP model for classification. Done after the other two completed.\n",
    "\n",
    "# Import the three packages and pickle\n",
    "import classes\n",
    "import nlp_model\n",
    "import pickle as pk\n",
    "import watson_nlu\n",
    "import sarcasm_detector\n",
    "import sys\n",
    "\n",
    "# Initiate this in the first place. Add to main\n",
    "nlu_api = watson_nlu.get_ibm_nlu()\n",
    "clf, tv, ps, scores = sarcasm_detector.get_model()\n",
    "\n",
    "\n",
    "def is_eligible(tweets):\n",
    "    \"\"\"Given list of tweets that fetched from search api to check if \n",
    "    they are eligible classification.\n",
    "    If you want to pass only one tweet, add brackets tweet -> [tweet]\n",
    "    \n",
    "    Params: \n",
    "        tweets: list of dictionaries taken from search api \n",
    "    Returns: \n",
    "        1-0 matrix of len(tweets)\n",
    "    \"\"\"\n",
    "    fear_matrix = [watson_nlu.is_fearful(tweet.text, nlu_api) for tweet in tweets]\n",
    "    processed_tweets = sarcasm_detector.preprocess(tweets, ps)\n",
    "    sarcasm_matrix = clf.predict(tv.transform(processed_tweets))\n",
    "    return [1 if (fear_matrix[i] and not sarcasm_matrix[i]) else 0 for i in range(len(fear_matrix))]\n",
    "\n",
    "def emergent_tweets(tweets):\n",
    "    \"\"\" Applies classification criteria to the tweets and returns only eligibles.\n",
    "    Returns: \n",
    "        Emergent tweets with text, id, and place vs. corresponding Class.\n",
    "    \"\"\"\n",
    "    eligible_tweets = zip([(tweet.text, tweet.id, tweet.place) for i, tweet in enumerate(tweets) if is_eligible(tweets)[i]])\n",
    "    \n",
    "    return eligible_tweets, nlp_model.classify_list(next(eligible_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-f1ffc218de78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memergent_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sda\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sdada police hack shot gun\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-51-e0cbab7cf5a2>\u001b[0m in \u001b[0;36memergent_tweets\u001b[0;34m(tweets)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mEmergent\u001b[0m \u001b[0mtweets\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mplace\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mClass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \"\"\"\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0meligible_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplace\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_eligible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0meligible_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meligible_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-e0cbab7cf5a2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mEmergent\u001b[0m \u001b[0mtweets\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mplace\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mClass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \"\"\"\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0meligible_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplace\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_eligible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0meligible_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meligible_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-e0cbab7cf5a2>\u001b[0m in \u001b[0;36mis_eligible\u001b[0;34m(tweets)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0mof\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \"\"\"\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mfear_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwatson_nlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fearful\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlu_api\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mprocessed_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msarcasm_detector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0msarcasm_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-e0cbab7cf5a2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0mof\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \"\"\"\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mfear_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwatson_nlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fearful\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlu_api\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mprocessed_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msarcasm_detector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0msarcasm_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "emergent_tweets([\"sda\", \"sdada police hack shot gun\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
